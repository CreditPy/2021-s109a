{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Homework 4:  Logistic Regression and PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2021**<br/>\n",
    "**Instructors**: Kevin Rader\n",
    "\n",
    "\n",
    "<hr style='height:2px'>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partners: Sunil Kumar Shambulingaiah and Steven Devisch** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# if you want to do a 2-sample t-test:\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Cancer Classification from Gene Expressions </div>\n",
    "\n",
    "In this problem, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The dataset is provided in the file `data/genomic_data.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with **0 indicating the ALL** class and **1 indicating the AML** class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will use linear and logistic regression to build classification models for this data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [15 pts]: Data Exploration </b></div>\n",
    "\n",
    "The first step is to split the observations into an approximate 75-25 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). It also prints the dataset's shape before splitting and after splitting. `Cancer_type` is our target column.\n",
    "\n",
    "\n",
    "**1.1** Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values**. Normalizing genomic data is a fairly standard first step.\n",
    "\n",
    "\n",
    "**1.2** The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.\n",
    "\n",
    "\n",
    "**1.3** Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.\n",
    "\n",
    "Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.\n",
    "\n",
    "**Hint:** You may use any reasonable approach to determine the `best_predictor`, but please use something very simple (whether taught in this class or elsewhere).\n",
    "\n",
    "\n",
    "**1.4** Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is to split the observations into an approximate 75-25 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits). Print dataset shape before splitting and after splitting. `Cancer_type` is our target column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df = pd.read_csv('data/genomic_data.csv', index_col=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'Cancer_type'], \n",
    "                                                         df.Cancer_type, test_size=0.25, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = df.Cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752, 7130)\n",
      "(564, 7129) (188, 7129) (564,) (188,)\n",
      "0.0    0.511968\n",
      "1.0    0.488032\n",
      "Name: Cancer_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(df.Cancer_type.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** there seem to be more columns than rows. The dataset does seem to be balanced, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>AFFX-BioB-5_st</th>\n",
       "      <th>...</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>564.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-117.907036</td>\n",
       "      <td>-163.527476</td>\n",
       "      <td>26.835753</td>\n",
       "      <td>210.370847</td>\n",
       "      <td>-248.657677</td>\n",
       "      <td>-406.595313</td>\n",
       "      <td>-28.764782</td>\n",
       "      <td>-190.080423</td>\n",
       "      <td>74.023263</td>\n",
       "      <td>94.750752</td>\n",
       "      <td>...</td>\n",
       "      <td>162.952887</td>\n",
       "      <td>698.676059</td>\n",
       "      <td>2.962852</td>\n",
       "      <td>415.726722</td>\n",
       "      <td>-50.791438</td>\n",
       "      <td>833.447504</td>\n",
       "      <td>329.612463</td>\n",
       "      <td>7.808718</td>\n",
       "      <td>520.229369</td>\n",
       "      <td>-24.088765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>169.792429</td>\n",
       "      <td>189.691312</td>\n",
       "      <td>205.798155</td>\n",
       "      <td>205.008971</td>\n",
       "      <td>209.564816</td>\n",
       "      <td>258.448200</td>\n",
       "      <td>541.128517</td>\n",
       "      <td>162.728073</td>\n",
       "      <td>175.291394</td>\n",
       "      <td>356.386595</td>\n",
       "      <td>...</td>\n",
       "      <td>164.794952</td>\n",
       "      <td>521.178135</td>\n",
       "      <td>151.324810</td>\n",
       "      <td>580.873231</td>\n",
       "      <td>106.535986</td>\n",
       "      <td>820.213770</td>\n",
       "      <td>370.829412</td>\n",
       "      <td>84.859351</td>\n",
       "      <td>936.957523</td>\n",
       "      <td>72.056218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-894.789505</td>\n",
       "      <td>-1095.737090</td>\n",
       "      <td>-645.439001</td>\n",
       "      <td>-425.750627</td>\n",
       "      <td>-1154.285694</td>\n",
       "      <td>-1091.492494</td>\n",
       "      <td>-2066.516448</td>\n",
       "      <td>-728.996262</td>\n",
       "      <td>-770.025847</td>\n",
       "      <td>-1176.663615</td>\n",
       "      <td>...</td>\n",
       "      <td>-416.206278</td>\n",
       "      <td>-1010.430536</td>\n",
       "      <td>-447.077819</td>\n",
       "      <td>-1870.955269</td>\n",
       "      <td>-432.967768</td>\n",
       "      <td>-2151.593119</td>\n",
       "      <td>-651.051441</td>\n",
       "      <td>-255.419724</td>\n",
       "      <td>-2754.665185</td>\n",
       "      <td>-237.864216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-225.038171</td>\n",
       "      <td>-274.322819</td>\n",
       "      <td>-95.270925</td>\n",
       "      <td>84.245857</td>\n",
       "      <td>-379.511872</td>\n",
       "      <td>-566.993723</td>\n",
       "      <td>-342.364586</td>\n",
       "      <td>-300.978001</td>\n",
       "      <td>-20.279803</td>\n",
       "      <td>-122.958169</td>\n",
       "      <td>...</td>\n",
       "      <td>68.056179</td>\n",
       "      <td>354.959208</td>\n",
       "      <td>-88.764266</td>\n",
       "      <td>96.448742</td>\n",
       "      <td>-105.297264</td>\n",
       "      <td>274.126036</td>\n",
       "      <td>89.320933</td>\n",
       "      <td>-41.845553</td>\n",
       "      <td>-49.150550</td>\n",
       "      <td>-73.781203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-107.412549</td>\n",
       "      <td>-146.993065</td>\n",
       "      <td>20.548933</td>\n",
       "      <td>213.143203</td>\n",
       "      <td>-228.639073</td>\n",
       "      <td>-432.747578</td>\n",
       "      <td>-49.602631</td>\n",
       "      <td>-179.079214</td>\n",
       "      <td>88.038576</td>\n",
       "      <td>102.087963</td>\n",
       "      <td>...</td>\n",
       "      <td>153.844059</td>\n",
       "      <td>679.200114</td>\n",
       "      <td>8.707703</td>\n",
       "      <td>379.423043</td>\n",
       "      <td>-35.816408</td>\n",
       "      <td>776.498538</td>\n",
       "      <td>290.209261</td>\n",
       "      <td>7.337169</td>\n",
       "      <td>460.622697</td>\n",
       "      <td>-25.003950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>-6.157261</td>\n",
       "      <td>-43.097489</td>\n",
       "      <td>127.413838</td>\n",
       "      <td>331.513199</td>\n",
       "      <td>-115.153022</td>\n",
       "      <td>-251.990616</td>\n",
       "      <td>324.333687</td>\n",
       "      <td>-73.891345</td>\n",
       "      <td>186.391086</td>\n",
       "      <td>284.218792</td>\n",
       "      <td>...</td>\n",
       "      <td>265.721675</td>\n",
       "      <td>1058.612890</td>\n",
       "      <td>95.688864</td>\n",
       "      <td>718.826611</td>\n",
       "      <td>19.827732</td>\n",
       "      <td>1321.287958</td>\n",
       "      <td>520.922602</td>\n",
       "      <td>62.512588</td>\n",
       "      <td>1109.915839</td>\n",
       "      <td>20.020152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>516.618491</td>\n",
       "      <td>384.626762</td>\n",
       "      <td>696.983127</td>\n",
       "      <td>1022.560525</td>\n",
       "      <td>308.876677</td>\n",
       "      <td>607.737258</td>\n",
       "      <td>1385.881293</td>\n",
       "      <td>263.634029</td>\n",
       "      <td>589.778466</td>\n",
       "      <td>1355.537180</td>\n",
       "      <td>...</td>\n",
       "      <td>686.321597</td>\n",
       "      <td>2522.188808</td>\n",
       "      <td>481.820516</td>\n",
       "      <td>3982.980299</td>\n",
       "      <td>215.047697</td>\n",
       "      <td>3743.264860</td>\n",
       "      <td>1996.244626</td>\n",
       "      <td>274.575647</td>\n",
       "      <td>3315.374276</td>\n",
       "      <td>224.773440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 7129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  AFFX-BioC-5_at  \\\n",
       "count      564.000000      564.000000      564.000000      564.000000   \n",
       "mean      -117.907036     -163.527476       26.835753      210.370847   \n",
       "std        169.792429      189.691312      205.798155      205.008971   \n",
       "min       -894.789505    -1095.737090     -645.439001     -425.750627   \n",
       "25%       -225.038171     -274.322819      -95.270925       84.245857   \n",
       "50%       -107.412549     -146.993065       20.548933      213.143203   \n",
       "75%         -6.157261      -43.097489      127.413838      331.513199   \n",
       "max        516.618491      384.626762      696.983127     1022.560525   \n",
       "\n",
       "       AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  AFFX-CreX-5_at  \\\n",
       "count      564.000000       564.000000       564.000000      564.000000   \n",
       "mean      -248.657677      -406.595313       -28.764782     -190.080423   \n",
       "std        209.564816       258.448200       541.128517      162.728073   \n",
       "min      -1154.285694     -1091.492494     -2066.516448     -728.996262   \n",
       "25%       -379.511872      -566.993723      -342.364586     -300.978001   \n",
       "50%       -228.639073      -432.747578       -49.602631     -179.079214   \n",
       "75%       -115.153022      -251.990616       324.333687      -73.891345   \n",
       "max        308.876677       607.737258      1385.881293      263.634029   \n",
       "\n",
       "       AFFX-CreX-3_at  AFFX-BioB-5_st  ...   U48730_at    U58516_at  \\\n",
       "count      564.000000      564.000000  ...  564.000000   564.000000   \n",
       "mean        74.023263       94.750752  ...  162.952887   698.676059   \n",
       "std        175.291394      356.386595  ...  164.794952   521.178135   \n",
       "min       -770.025847    -1176.663615  ... -416.206278 -1010.430536   \n",
       "25%        -20.279803     -122.958169  ...   68.056179   354.959208   \n",
       "50%         88.038576      102.087963  ...  153.844059   679.200114   \n",
       "75%        186.391086      284.218792  ...  265.721675  1058.612890   \n",
       "max        589.778466     1355.537180  ...  686.321597  2522.188808   \n",
       "\n",
       "        U73738_at    X06956_at   X16699_at    X83863_at    Z17240_at  \\\n",
       "count  564.000000   564.000000  564.000000   564.000000   564.000000   \n",
       "mean     2.962852   415.726722  -50.791438   833.447504   329.612463   \n",
       "std    151.324810   580.873231  106.535986   820.213770   370.829412   \n",
       "min   -447.077819 -1870.955269 -432.967768 -2151.593119  -651.051441   \n",
       "25%    -88.764266    96.448742 -105.297264   274.126036    89.320933   \n",
       "50%      8.707703   379.423043  -35.816408   776.498538   290.209261   \n",
       "75%     95.688864   718.826611   19.827732  1321.287958   520.922602   \n",
       "max    481.820516  3982.980299  215.047697  3743.264860  1996.244626   \n",
       "\n",
       "       L49218_f_at  M71243_f_at  Z78285_f_at  \n",
       "count   564.000000   564.000000   564.000000  \n",
       "mean      7.808718   520.229369   -24.088765  \n",
       "std      84.859351   936.957523    72.056218  \n",
       "min    -255.419724 -2754.665185  -237.864216  \n",
       "25%     -41.845553   -49.150550   -73.781203  \n",
       "50%       7.337169   460.622697   -25.003950  \n",
       "75%      62.512588  1109.915839    20.020152  \n",
       "max     274.575647  3315.374276   224.773440  \n",
       "\n",
       "[8 rows x 7129 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 7129)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>AFFX-BioB-5_st</th>\n",
       "      <th>...</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.619351</td>\n",
       "      <td>0.945349</td>\n",
       "      <td>0.817396</td>\n",
       "      <td>0.841058</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>0.360329</td>\n",
       "      <td>0.461770</td>\n",
       "      <td>0.601425</td>\n",
       "      <td>0.827058</td>\n",
       "      <td>0.225030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685016</td>\n",
       "      <td>0.504484</td>\n",
       "      <td>0.469944</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.722699</td>\n",
       "      <td>0.631021</td>\n",
       "      <td>0.216853</td>\n",
       "      <td>0.742124</td>\n",
       "      <td>0.312083</td>\n",
       "      <td>0.621290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.493430</td>\n",
       "      <td>0.593738</td>\n",
       "      <td>0.582163</td>\n",
       "      <td>0.475156</td>\n",
       "      <td>0.784989</td>\n",
       "      <td>0.183778</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.795903</td>\n",
       "      <td>0.255178</td>\n",
       "      <td>0.549755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449506</td>\n",
       "      <td>0.303861</td>\n",
       "      <td>0.565232</td>\n",
       "      <td>0.494874</td>\n",
       "      <td>0.608248</td>\n",
       "      <td>0.480530</td>\n",
       "      <td>0.233495</td>\n",
       "      <td>0.704078</td>\n",
       "      <td>0.754073</td>\n",
       "      <td>0.485576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.453711</td>\n",
       "      <td>0.638256</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.665891</td>\n",
       "      <td>0.515745</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.496062</td>\n",
       "      <td>0.488115</td>\n",
       "      <td>0.730907</td>\n",
       "      <td>0.546633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899386</td>\n",
       "      <td>0.550748</td>\n",
       "      <td>0.113228</td>\n",
       "      <td>0.094507</td>\n",
       "      <td>0.261622</td>\n",
       "      <td>0.418446</td>\n",
       "      <td>0.551573</td>\n",
       "      <td>0.821525</td>\n",
       "      <td>0.633380</td>\n",
       "      <td>0.623126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497778</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.426631</td>\n",
       "      <td>0.408176</td>\n",
       "      <td>0.574250</td>\n",
       "      <td>0.315588</td>\n",
       "      <td>0.691303</td>\n",
       "      <td>0.698345</td>\n",
       "      <td>0.819778</td>\n",
       "      <td>0.446504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558269</td>\n",
       "      <td>0.372834</td>\n",
       "      <td>0.507707</td>\n",
       "      <td>0.391318</td>\n",
       "      <td>0.598964</td>\n",
       "      <td>0.568479</td>\n",
       "      <td>0.318087</td>\n",
       "      <td>0.495726</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.486846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.572308</td>\n",
       "      <td>0.685343</td>\n",
       "      <td>0.413806</td>\n",
       "      <td>0.432270</td>\n",
       "      <td>0.619183</td>\n",
       "      <td>0.612995</td>\n",
       "      <td>0.554375</td>\n",
       "      <td>0.674835</td>\n",
       "      <td>0.640734</td>\n",
       "      <td>0.475720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760584</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>0.433381</td>\n",
       "      <td>0.508760</td>\n",
       "      <td>0.693013</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.466636</td>\n",
       "      <td>0.561463</td>\n",
       "      <td>0.507653</td>\n",
       "      <td>0.467111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  AFFX-BioC-5_at  \\\n",
       "0        0.619351        0.945349        0.817396        0.841058   \n",
       "1        0.493430        0.593738        0.582163        0.475156   \n",
       "2        0.453711        0.638256        0.737430        0.665891   \n",
       "3        0.497778        0.726115        0.426631        0.408176   \n",
       "4        0.572308        0.685343        0.413806        0.432270   \n",
       "\n",
       "   AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  AFFX-CreX-5_at  \\\n",
       "0        0.683758         0.360329         0.461770        0.601425   \n",
       "1        0.784989         0.183778         0.792969        0.795903   \n",
       "2        0.515745         0.427893         0.496062        0.488115   \n",
       "3        0.574250         0.315588         0.691303        0.698345   \n",
       "4        0.619183         0.612995         0.554375        0.674835   \n",
       "\n",
       "   AFFX-CreX-3_at  AFFX-BioB-5_st  ...  U48730_at  U58516_at  U73738_at  \\\n",
       "0        0.827058        0.225030  ...   0.685016   0.504484   0.469944   \n",
       "1        0.255178        0.549755  ...   0.449506   0.303861   0.565232   \n",
       "2        0.730907        0.546633  ...   0.899386   0.550748   0.113228   \n",
       "3        0.819778        0.446504  ...   0.558269   0.372834   0.507707   \n",
       "4        0.640734        0.475720  ...   0.760584   0.516456   0.433381   \n",
       "\n",
       "   X06956_at  X16699_at  X83863_at  Z17240_at  L49218_f_at  M71243_f_at  \\\n",
       "0   0.105544   0.722699   0.631021   0.216853     0.742124     0.312083   \n",
       "1   0.494874   0.608248   0.480530   0.233495     0.704078     0.754073   \n",
       "2   0.094507   0.261622   0.418446   0.551573     0.821525     0.633380   \n",
       "3   0.391318   0.598964   0.568479   0.318087     0.495726     0.505263   \n",
       "4   0.508760   0.693013   0.509524   0.466636     0.561463     0.507653   \n",
       "\n",
       "   Z78285_f_at  \n",
       "0     0.621290  \n",
       "1     0.485576  \n",
       "2     0.623126  \n",
       "3     0.486846  \n",
       "4     0.467111  \n",
       "\n",
       "[5 rows x 7129 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# archive column names as scale_transformer produces an array, removing column names\n",
    "columns = X_train.columns\n",
    "# scale the datasets\n",
    "# use minmaxscalar as some predictors are negative\n",
    "scale_transformer = MinMaxScaler(copy=True).fit(X_train)\n",
    "X_train = pd.DataFrame(scale_transformer.transform(X_train))\n",
    "X_test = pd.DataFrame(scale_transformer.transform(X_test))\n",
    "\n",
    "# reapply columns\n",
    "X_train.columns = columns\n",
    "X_test.columns = columns\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely the predictors are correlated. The parameters are unidentifiable. Predictions on the training set will be perfect, but not on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.**\n",
    "\n",
    "**Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "predictors = X_train.columns\n",
    "for predictor in predictors:\n",
    "    \n",
    "    logref = LogisticRegression(solver='liblinear', max_iter=1000, C=100000)\n",
    "    logref.fit(X_train[predictor].values.reshape(-1, 1), y_train)\n",
    "    y_hat_test = logref.predict(X_test[predictor].values.reshape(-1, 1))\n",
    "    cur_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_model = logref\n",
    "        best_predictor = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found logistic regression model: LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "Accuracy score: 0.7340425531914894\n"
     ]
    }
   ],
   "source": [
    "print(\"Best found logistic regression model:\", best_model,\"\\nAccuracy score:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best found logistic regression model uses this variable: Y08612_at\n"
     ]
    }
   ],
   "source": [
    "print(\"The best found logistic regression model uses this variable:\", best_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Histogram comparions between train and test set for best predictor: Y08612_at')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2 graphs side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# historgram on training set \n",
    "ax[0].hist(X_train[best_predictor][y_train.values==1], alpha=0.5, label = 'AML')\n",
    "ax[0].hist(X_train[best_predictor][y_train.values==0], alpha=0.5, label = 'ALL')\n",
    "ax[0].set_xlabel('Standardized predictor ' +  best_predictor)\n",
    "ax[0].set_ylabel('Number of cancer cases')\n",
    "ax[0].set_title('AML and ALL distribution in train set by values of predictor ' +  best_predictor)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(X_test[best_predictor][y_test.values==1], alpha=0.5, label = 'AML')\n",
    "ax[1].hist(X_test[best_predictor][y_test.values==0], alpha=0.5, label = 'ALL')\n",
    "ax[1].set_xlabel('Standardized predictor ' +  best_predictor)\n",
    "ax[1].set_ylabel('Number of cancer cases')\n",
    "ax[1].set_title('AML and ALL distribution in test set by values of predictor ' +  best_predictor)\n",
    "ax[1].legend()\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "fig.suptitle(\"Histogram comparions between train and test set for best predictor: \" + best_predictor, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The histograms for bot the train and test set demonstrat how the best predictor correlates more with ALL at higher values of the predictor, and more with AML at lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eye-ball model has an accuracy of: 0.3617021276595745 on the test set\n"
     ]
    }
   ],
   "source": [
    "# default to AML\n",
    "y_hat_test[:] = 0\n",
    "\n",
    "# set threshold for ALL above 0.5\n",
    "thresh = 0.4\n",
    "ALL_indices = X_test[best_predictor].values > thresh\n",
    "y_hat_test[ALL_indices] = 1\n",
    "\n",
    "eyeball_accuracy = accuracy_score(y_hat_test, y_test.to_numpy())\n",
    "print(\"The eye-ball model has an accuracy of: \" + str(eyeball_accuracy) + \" on the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 2 [35pts]: Logistic Regression Modeling </b> </div>\n",
    "\n",
    "\n",
    "**2.1** Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type.  Carefully interpret the coefficient estimates for this model.\n",
    "\n",
    "*Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to **not** regularize (use `C=100000` or `penalty = \"none\"`).\n",
    "\n",
    "**2.2** Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?\n",
    "\n",
    "**2.3** Calculate the training and test classification accuracies of this model in 2.1. How do these compare to the eye-balled model from 1.4?\n",
    "\n",
    "\n",
    "**2.4** Next, fit a multiple logistic regression model with **all** the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?  \n",
    "\n",
    "**2.5** Print out and interpret the logistic regression coefficients for  `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?\n",
    "\n",
    "**2.6** Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.\n",
    "\n",
    "**2.7** How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type. Carefully interpret the coefficient estimates for this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a model with 'best' predictor Y08612_at is: 0.7340425531914894\n",
      "[[-6.0619642]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', max_iter=1000, C=100000)\n",
    "logreg.fit(X_train[best_predictor].values.reshape(-1, 1), y_train)\n",
    "y_hat_test = logreg.predict(X_test[best_predictor].values.reshape(-1, 1))\n",
    "cur_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "print(\"Accuracy for a model with 'best' predictor \" + best_predictor + \" is: \" + str(cur_accuracy))\n",
    "print(logref.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odds = np.exp(logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient interpretation:** For every one-unit increase in the best predictor 'Y08612_at', the odds that the observation is in AML are 0.00233 times as large as the odds that the observation is not in AML when all other variables are held constant.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Calculate the training and test classification accuracies of this model in 2.1.  How do these compare to the eye-balled model from 1.4?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Next, fit a multiple logistic regression model with *all* the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Print out and interpret the logistic regression coefficients for `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [10pts]: $k$-NN Classification </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`. \n",
    "\n",
    "**3.2** Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. Report the false positive and false negative rates (all in the test set).  Briefly interpret what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`.  Report your chosen $k$, and report the misclassification rate on both the train and test sets for the model using your chosen $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. what are the  false positive and false negative rates in these 3 models (all in the test set)?  Briefly interpret what you notice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class='exercise'><b> Question 4 [15 pts]: Performing Principal Components Analysis </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Create the full PCA decomposition of `X_train` and apply the transformation to both `X_train` and `X_test`.  Report the shape of both of these.  What is the limiting factor for the maximum number of PCA components for this data set? \n",
    "\n",
    "*Hint: be sure to standardize before performing PCA.\n",
    "\n",
    "**4.2** PCA is often solely used to help in visualizing high-dimensional problems.  Plot the scatterplot of the second PCA vector of train on the $Y$-axis and the first PCA vector of train on the $X$-axis (be sure to denote the classes via different colors and markings).  In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.\n",
    "\n",
    "**4.3** Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Create the full PCA decomposition of X_train and apply the transformation to both X_train and X_test. Report the shape of both of these. What is the limiting factor for the maximum number of PCA components for this data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 PCA is often solely used to help in visualizing high-dimensional problems. Plot the scatterplot of the second PCA vector on the  𝑌 -axis and the first PCA vector on the  𝑋 -axis (be sure to denote the classes via different color/markings). In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 5 [10 pts]: Principal Components Regression (PCR) </b></div>\n",
    "\n",
    "**5.1** Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?\n",
    "\n",
    "**5.2** Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between `best_predictor` and `Cancer_type`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between your `best_predictor` and `Cancer_type`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 6 [15 pts]: Evaluating Classifiers </b></div>\n",
    "\n",
    "**6.1**: Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).\n",
    "\n",
    "**6.2** Evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models using AUC.  That is calculate AUC for the following models:\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "For the model with the best AUC, plot the ROC. Briefly interpret your plot.\n",
    "\n",
    "**6.3** Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?\n",
    "\n",
    "*See extra information about ALL and AML at the bottom of this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 use AUC to evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models.  That is calculate AUC for the following models:**\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "**For the model with the best AUC, plot the ROC. Briefly interpret your plot.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3 Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Information**\n",
    "\n",
    "Acute Lymphoblastic Leukemia (ALL):\n",
    "- About 98% of children with ALL go into remission within weeks after starting treatment.\n",
    "- About 90% of those children can be cured. Patients are considered cured after 10 years in remission.\n",
    "\n",
    "Acute Myeloid Leukemia (AML):\n",
    "- In general, children with AML are seen as lower risk than adults. \n",
    "- Around 85 to 90 percent of children with AML will go into remission after induction, according to the American Cancer Society. AML will return in some cases.  \n",
    "- The five-year-survival-rate for children with AML is 60 to 70 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
