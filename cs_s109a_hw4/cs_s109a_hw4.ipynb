{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Homework 4:  Logistic Regression and PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2021**<br/>\n",
    "**Instructors**: Kevin Rader\n",
    "\n",
    "\n",
    "<hr style='height:2px'>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partners: Sunil Kumar Shambulingaiah and Steven Devisch** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# if you want to do a 2-sample t-test:\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Cancer Classification from Gene Expressions </div>\n",
    "\n",
    "In this problem, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The dataset is provided in the file `data/genomic_data.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with **0 indicating the ALL** class and **1 indicating the AML** class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will use linear and logistic regression to build classification models for this data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [15 pts]: Data Exploration </b></div>\n",
    "\n",
    "The first step is to split the observations into an approximate 75-25 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). It also prints the dataset's shape before splitting and after splitting. `Cancer_type` is our target column.\n",
    "\n",
    "\n",
    "**1.1** Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values**. Normalizing genomic data is a fairly standard first step.\n",
    "\n",
    "\n",
    "**1.2** The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.\n",
    "\n",
    "\n",
    "**1.3** Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.\n",
    "\n",
    "Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.\n",
    "\n",
    "**Hint:** You may use any reasonable approach to determine the `best_predictor`, but please use something very simple (whether taught in this class or elsewhere).\n",
    "\n",
    "\n",
    "**1.4** Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is to split the observations into an approximate 75-25 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits). Print dataset shape before splitting and after splitting. `Cancer_type` is our target column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df = pd.read_csv('data/genomic_data.csv', index_col=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'Cancer_type'], \n",
    "                                                         df.Cancer_type, test_size=0.25, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = df.Cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752, 7130)\n",
      "(564, 7129) (188, 7129) (564,) (188,)\n",
      "0.0    0.511968\n",
      "1.0    0.488032\n",
      "Name: Cancer_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(df.Cancer_type.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** there seem to be more columns than rows. The dataset does seem to be balanced, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive column names as scale_transformer produces an array, removing column names\n",
    "columns = X_train.columns\n",
    "# scale the datasets\n",
    "# use minmaxscalar as some predictors are negative\n",
    "scale_transformer = MinMaxScaler(copy=True).fit(X_train)\n",
    "X_train = pd.DataFrame(scale_transformer.transform(X_train))\n",
    "X_test = pd.DataFrame(scale_transformer.transform(X_test))\n",
    "\n",
    "# reapply columns\n",
    "X_train.columns = columns\n",
    "X_test.columns = columns\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely the predictors are correlated. The parameters are unidentifiable. Predictions on the training set will be perfect, but not on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.**\n",
    "\n",
    "**Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "predictors = X_train.columns\n",
    "for predictor in predictors:\n",
    "    \n",
    "    logref = LogisticRegression(solver='liblinear', max_iter=1000, C=100000)\n",
    "    logref.fit(X_train[predictor].values.reshape(-1, 1), y_train)\n",
    "    y_hat_test = logref.predict(X_test[predictor].values.reshape(-1, 1))\n",
    "    cur_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_model = logref\n",
    "        best_predictor = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best found logistic regression model:\", best_model,\"\\nAccuracy score:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best found logistic regression model uses this variable:\", best_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 graphs side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# historgram on training set \n",
    "ax[0].hist(X_train[best_predictor][y_train.values==1], alpha=0.5, label = 'AML')\n",
    "ax[0].hist(X_train[best_predictor][y_train.values==0], alpha=0.5, label = 'ALL')\n",
    "ax[0].set_xlabel('Standardized predictor ' +  best_predictor)\n",
    "ax[0].set_ylabel('Number of cancer cases')\n",
    "ax[0].set_title('AML and ALL distribution in train set by values of predictor ' +  best_predictor)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(X_test[best_predictor][y_test.values==1], alpha=0.5, label = 'AML')\n",
    "ax[1].hist(X_test[best_predictor][y_test.values==0], alpha=0.5, label = 'ALL')\n",
    "ax[1].set_xlabel('Standardized predictor ' +  best_predictor)\n",
    "ax[1].set_ylabel('Number of cancer cases')\n",
    "ax[1].set_title('AML and ALL distribution in test set by values of predictor ' +  best_predictor)\n",
    "ax[1].legend()\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "fig.suptitle(\"Histogram comparions between train and test set for best predictor: \" + best_predictor, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The histograms for bot the train and test set demonstrat how the best predictor correlates more with ALL at higher values of the predictor, and more with AML at lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default to ALL\n",
    "y_hat_test[:] = 0\n",
    "\n",
    "# set threshold for ALL above 0.5\n",
    "thresh = 0.5\n",
    "AML_indices = X_test[best_predictor].values > thresh\n",
    "# set to AML if predictor exceeds the threshold value\n",
    "y_hat_test[AML_indices] = 1\n",
    "\n",
    "#@Sunil something's wrong in this calculation. Accuracy should be closer to 66% (see below)\n",
    "eyeball_accuracy = accuracy_score(y_hat_test, y_test.to_numpy())\n",
    "print(\"The eye-ball model has an accuracy of: \" + str(eyeball_accuracy) + \" on the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 2 [35pts]: Logistic Regression Modeling </b> </div>\n",
    "\n",
    "\n",
    "**2.1** Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type.  Carefully interpret the coefficient estimates for this model.\n",
    "\n",
    "*Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to **not** regularize (use `C=100000` or `penalty = \"none\"`).\n",
    "\n",
    "**2.2** Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?\n",
    "\n",
    "**2.3** Calculate the training and test classification accuracies of this model in 2.1. How do these compare to the eye-balled model from 1.4?\n",
    "\n",
    "\n",
    "**2.4** Next, fit a multiple logistic regression model with **all** the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?  \n",
    "\n",
    "**2.5** Print out and interpret the logistic regression coefficients for  `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?\n",
    "\n",
    "**2.6** Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.\n",
    "\n",
    "**2.7** How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type. Carefully interpret the coefficient estimates for this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', max_iter=1000, C=100000)\n",
    "logreg.fit(X_train[best_predictor].values.reshape(-1, 1), y_train)\n",
    "y_hat_test = logreg.predict(X_test[best_predictor].values.reshape(-1, 1))\n",
    "cur_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "print(\"Accuracy for a model with 'best' predictor \" + best_predictor + \" is: \" + str(cur_accuracy))\n",
    "print(logref.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert log odds into odds for the coeffinet associated with the best predictor\n",
    "odds = np.exp(logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient interpretation:** For every one-unit increase in the best predictor 'Y08612_at', the odds that the observation is in AML are 0.00233 times as large as the odds that the observation is not in AML when all other variables are held constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn provides a convenient method to plotting logistic curves\n",
    "sbplt = sns.regplot(x=X_train[best_predictor], y=y_train.values, logistic=True)\n",
    "# add a horizontal line at the cut-off point\n",
    "sbplt.axhline(0.5, c='red', alpha=0.5)\n",
    "sbplt.set(xlabel='Best predictor: ' + best_predictor, ylabel='Probability', title='Logistic curve for the best predictor model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We observe that the prediction flips at a Y08612_at value of a little bit below 0.5. This is similar to our ealier guess. At values below 0.5 for standardized Y08612_at, we predict a patient to have ALL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Calculate the training and test classification accuracies of this model in 2.1.  How do these compare to the eye-balled model from 1.4?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate training accuracies\n",
    "y_hat_train = logreg.predict(X_train[best_predictor].values.reshape(-1, 1))\n",
    "train_accuracy = accuracy_score(y_train.to_numpy(), y_hat_train)\n",
    "print(\"Accuracy on the training set is:\", train_accuracy)\n",
    "\n",
    "# calculate test accuracies\n",
    "y_hat_test = logreg.predict(X_test[best_predictor].values.reshape(-1, 1))\n",
    "test_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "print(\"Accuracy on the test set is:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The eyeballed model and the logistic regression model perform similarly. This seems reasonable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Next, fit a multiple logistic regression model with *all* the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with all predictors (without regularizing)\n",
    "logreg_all = LogisticRegression(solver='liblinear', max_iter=1000, C=100000)\n",
    "logreg_all.fit(X_train, y_train)\n",
    "\n",
    "# calculate training accuracies\n",
    "y_hat_train = logreg_all.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train.to_numpy(), y_hat_train)\n",
    "print(\"Accuracy on the training set is:\", train_accuracy)\n",
    "\n",
    "# calculate test accuracies\n",
    "y_hat_test_all = logreg_all.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test_all)\n",
    "print(\"Accuracy on the test set is:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** As anticipated above since there are more predictors than observations, model accuracy for the training set becomes 100%. Model accuracy for the test set has increased as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Print out and interpret the logistic regression coefficients for `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_all.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the position of the best predictor\n",
    "best_pred_pos = X_train.columns.get_loc(best_predictor)\n",
    "\n",
    "# coef_list = np.ndarray.tolist(logreg_all.coef_)\n",
    "# coef_list[0][best_pred_pos]\n",
    "print('The coefficient for best_predictor in simple logistic regression is:', logref.coef_[0][0])\n",
    "print('The coefficient for best_predictor in multiple logistic regression is:', logreg_all.coef_[0][best_pred_pos])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The coefficients for simple and multiple logistic regression are very different. This is likely due to high collinearity between the different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "accuracies = []\n",
    "\n",
    "# experiment with different values of different order of magnitudes\n",
    "cs_vals = np.ndarray.tolist((np.ones(20)*10)**np.arange(-10,10,1).astype(int))\n",
    "# overriding cs_vals as only integers work\n",
    "cs_vals = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100,1000]\n",
    "# @ Sunil. subseting cvals to lower number as 20 take long to compute. Put back befor submitting.\n",
    "cs_vals = [4,5,6]\n",
    "\n",
    "# adding a solver choice in case we also want to experiment with different solvers\n",
    "solvers = ['liblinear']\n",
    "for c in cs_vals:\n",
    "    for sol in solvers:\n",
    "        # use l1 for lasso and 5 cross validation (cv) folds\n",
    "        logref =  LogisticRegressionCV(Cs=c, solver=sol, penalty='l1', cv=5)\n",
    "        logref.fit(X_train, y_train)\n",
    "        y_hat_test = logref.predict(X_test)\n",
    "        cur_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "        \n",
    "        # adding accuracy to a list in case we want to show how accuracy changes with lambda\n",
    "        accuracies.append(cur_accuracy)\n",
    "        if cur_accuracy > best_accuracy:\n",
    "            best_accuracy = cur_accuracy\n",
    "            best_reg_model = logref\n",
    "            best_lambda = c\n",
    "            \n",
    "# predictions\n",
    "y_train_pred = best_reg_model.predict(X_train) \n",
    "y_test_pred = best_reg_model.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_score = accuracy_score(y_train, y_train_pred)\n",
    "test_score = accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best lambda was:\", best_lambda)\n",
    "print(\"Errors on the training set were:\", train_score)\n",
    "print(\"Errors on the test set were:\",test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: after regularization, test scores improved significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of non-zero coeffients in the model\n",
    "print(\"The number of non-zero coefficients in the model is:\", sum(x > 0 for x in best_reg_model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** The regularized model retained nineteen predictors. We can conclude that the initial predictor set - as used in the full logistic regression model - was highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [10pts]: $k$-NN Classification </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`. \n",
    "\n",
    "**3.2** Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. Report the false positive and false negative rates (all in the test set).  Briefly interpret what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`.  Report your chosen $k$, and report the misclassification rate on both the train and test sets for the model using your chosen $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1,3,5,7,10,15,20,50,100]\n",
    "cv_folds = 5\n",
    "best_accuracy = -1\n",
    "acc = []\n",
    "\n",
    "for k in ks:\n",
    "    knn_model = KNeighborsClassifier(k)\n",
    "    cur_accuracy = np.mean(cross_val_score(knn_model, X_train, y_train, cv=cv_folds))\n",
    "    acc.append(cur_accuracy)\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_knn_model = knn_model \n",
    "        best_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))    \n",
    "plt.plot(ks, acc)\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('Classification accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best value for k is: \", best_k)\n",
    "print(\"Classification accuracy for this k:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. what are the  false positive and false negative rates in these 3 models (all in the test set)?  Briefly interpret what you notice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calculate false positive and false positive rates\n",
    "def calc_confusion_rate(confusion_matrix):\n",
    "        TP = confusion_matrix.loc['true:0','pred:0']\n",
    "        TN =  confusion_matrix.loc['true:1','pred:1']        \n",
    "        FP =  confusion_matrix.loc['true:1','pred:0']\n",
    "        FN =  confusion_matrix.loc['true:0','pred:1']\n",
    "        FP_rate = FP / (FP+TN)\n",
    "        FN_rate = FN / (FN+TP)\n",
    "        return FP_rate, FN_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the set of models and rates we seek to explore\n",
    "models = [logreg_all, best_reg_model, best_knn_model]\n",
    "model_descriptions = ['Full_logistic', 'Regularized_bLogistic', 'KNN']\n",
    "confusion_rates = ['False_Positive_Rate', 'False_Negative_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataframe to represent rates for each model\n",
    "confusion_rate_df = pd.DataFrame(\n",
    "    index=model_descriptions,\n",
    "    columns=['False_Positive_Rate', 'False_Negative_Rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for model in models:\n",
    "    # confusion matrix for model with all predictors\n",
    "    model.fit(X_train,y_train)\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    conf_df = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_hat_test), \n",
    "        index=['true:0', 'true:1'], \n",
    "        columns=['pred:0', 'pred:1'])\n",
    "    curr_model_descr = model_descriptions[index]\n",
    "    print(\"\\n Confusion matrix for:\", curr_model_descr,\"\\n\")\n",
    "\n",
    "    # calculate false positive and false negative rate\n",
    "    FP_rate, FN_rate = calc_confusion_rate(conf_df)\n",
    "    \n",
    "    # Add rates to dataframe for clear comparison \n",
    "    confusion_rate_df.loc[curr_model_descr,'False_Positive_Rate'] = FP_rate\n",
    "    confusion_rate_df.loc[curr_model_descr,'False_Negative_Rate'] = FN_rate\n",
    "    print(conf_df)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rates by model in percentage format\n",
    "confusion_rate_df.style.format({\n",
    "    'False_Positive_Rate': '{:,.1%}'.format,\n",
    "    'False_Negative_Rate': '{:,.1%}'.format,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** Both false positive and false negative rate are lowest for regularized logistic regression model. It is therefore reasonable to conclude that the regularized logistic regression model is the best model. K nearest neighbor also performs better than the full logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class='exercise'><b> Question 4 [15 pts]: Performing Principal Components Analysis </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Create the full PCA decomposition of `X_train` and apply the transformation to both `X_train` and `X_test`.  Report the shape of both of these.  What is the limiting factor for the maximum number of PCA components for this data set? \n",
    "\n",
    "*Hint: be sure to standardize before performing PCA.\n",
    "\n",
    "**4.2** PCA is often solely used to help in visualizing high-dimensional problems.  Plot the scatterplot of the second PCA vector of train on the $Y$-axis and the first PCA vector of train on the $X$-axis (be sure to denote the classes via different colors and markings).  In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.\n",
    "\n",
    "**4.3** Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Create the full PCA decomposition of X_train and apply the transformation to both X_train and X_test. Report the shape of both of these. What is the limiting factor for the maximum number of PCA components for this data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/fit the 'full' pca transformation for X_train\n",
    "pca_train = PCA().fit(X_train)\n",
    "\n",
    "# apply the pca transformation to the full predictor set\n",
    "pcaX_train = pca_train.transform(X_train)\n",
    "\n",
    "# convert to a data frame\n",
    "pcaX_df_train = pd.DataFrame(pcaX_train)\n",
    "\n",
    "print(\"Original train dataset shape:\",pca_train.components_.shape);\n",
    "print(\"Shape PCA transformed X_train:\", pcaX_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/fit the 'full' pca transformation for X_test\n",
    "pca_test = PCA().fit(X_test)\n",
    "\n",
    "# apply the pca transformation to the full predictor set\n",
    "pcaX_test = pca_test.transform(X_test)\n",
    "\n",
    "# convert to a data frame\n",
    "pcaX_df_test = pd.DataFrame(pcaX_test)\n",
    "\n",
    "print(\"Original test dataset shape:\",pca_test.components_.shape);\n",
    "print(\"Shape PCA transformed X_test:\", pcaX_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Sunil. I'm not sure if I'm answering the question.\n",
    "**Comment\"** If the number of observations ð‘› is less than or equal to the number of features, the ð‘›-th Principal Component (PC) will be constant zero (eigenvalue = 0). The number of non-trivial PCs is ð‘›âˆ’1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 PCA is often solely used to help in visualizing high-dimensional problems. Plot the scatterplot of the second PCA vector on the  ð‘Œ -axis and the first PCA vector on the  ð‘‹ -axis (be sure to denote the classes via different color/markings). In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the indexes. @Sunil I don't understand why I have to do this\n",
    "# without this, I get an error\n",
    "pcaX_df_train.index = y_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the response over the first 2 PCA component vectors\n",
    "plt.scatter(pcaX_df_train.iloc[:, 0][y_train==0],pcaX_df_train.iloc[:, 1][y_train==0])\n",
    "plt.scatter(pcaX_df_train.iloc[:, 0][y_train==1],pcaX_df_train.iloc[:, 1][y_train==1])\n",
    "\n",
    "plt.legend([\"ALL\",\"AML\"])\n",
    "plt.xlabel(\"First PCA Component Vector (Z1)\")\n",
    "plt.ylabel(\"Second PCA Component Vector (Z2)\")\n",
    "plt.title(\"ALL and AML as a function of top 2 principal components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Sunil. I suspect this explanation could be improved.\n",
    "**Comment** Most useful visualizations are limited to two dimensions. Visualizing the effect of a high number of predictors is therefore challenging. PCA allows to summarize the effect of multiple predictors in a a summarized 2-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance explained by fist two components\n",
    "print(\"Variance explained by the first principal component:\",pca_train.explained_variance_ratio_[0])\n",
    "print(\"Variance explained by the second principal component:\",pca_train.explained_variance_ratio_[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variability_explained = .9\n",
    "cum_PCA = np.cumsum(pca_train.explained_variance_ratio_)\n",
    "\n",
    "# asses at which point we reach a certain percentage for variability explained\n",
    "target_variability_at = len(cum_PCA) - sum(cum_PCA > target_variability_explained)\n",
    "\n",
    "print('Number of principal components needed to explain', \n",
    "      target_variability_explained, \"% of variability:\",\n",
    "      target_variability_at )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cumulative \"variance explained\" curve for X_train\n",
    "plt.plot(np.cumsum(pca_train.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.suptitle('number of principal componenets needed for 90% variance ')\n",
    "plt.axhline(0.9, c='red', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** A reasonable number of components seems to be any number between 75 and 400. For parsimony reasons, I would recommend 75, roughly the point where the curve starts flattening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 5 [10 pts]: Principal Components Regression (PCR) </b></div>\n",
    "\n",
    "**5.1** Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?\n",
    "\n",
    "**5.2** Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between `best_predictor` and `Cancer_type`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the set of models and rates we seek to explore\n",
    "pca_cnts = [2, 103, 75]\n",
    "model_descriptions = ['2_PCA_vectors', 'Ninety_perc_pca_vectors (103)', 'Chosen_pca_vectors (75)']\n",
    "\n",
    "best_accuracy = -1\n",
    "best_pca_model = None\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataframe to represent accuracy for each model\n",
    "pca_cnt_acc_df = pd.DataFrame(\n",
    "    index=model_descriptions,\n",
    "    columns=['Train_accuracy', 'Test_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_counter = 0\n",
    "for pca_cnts in pca_cnts:\n",
    "    # setting c high to avoid additional regularizatoin\n",
    "    logreg =  LogisticRegression(C=1000000, solver='liblinear', max_iter=1000)\n",
    "    # note we're using the pcaX dataset, subsetted to include the specific feature nr.\n",
    "    logreg.fit(pcaX_df_train.iloc[:,:pca_cnts], y_train)\n",
    "\n",
    "    \n",
    "    y_hat_train = logreg.predict(pcaX_df_train.iloc[:,:pca_cnts])\n",
    "    train_accuracy = accuracy_score(y_train.to_numpy(), y_hat_train)\n",
    "\n",
    "    y_hat_test = logreg.predict(pcaX_df_test.iloc[:,:pca_cnts])\n",
    "    test_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "\n",
    "    # adding accuracy to a list in case we want to show how accuracy changes with pca_cnts\n",
    "    accuracies.append(cur_accuracy)\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_pca_model = logreg\n",
    "        best_pca_cnt = pca_cnts\n",
    "    \n",
    "    # Add rates to dataframe for clear comparison \n",
    "    curr_model_descr = model_descriptions[iteration_counter]\n",
    "    pca_cnt_acc_df.loc[curr_model_descr,'Train_accuracy'] = train_accuracy\n",
    "    pca_cnt_acc_df.loc[curr_model_descr,'Test_accuracy'] = test_accuracy\n",
    "    iteration_counter = iteration_counter + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rates by model in percentage format\n",
    "print(\"Comparing different regression models by varying the number of principal components\")\n",
    "pca_cnt_acc_df.style.format({\n",
    "    'Train_accuracy': '{:,.1%}'.format,\n",
    "    'Test_accuracy': '{:,.1%}'.format,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** The 2-component model performs nearly as well as the chosen pca model. Since the two-component model is significantly more parsimoneous, it seems reasonable to prefer the two-component model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between your `best_predictor` and `Cancer_type`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cnts = [2, 10, 20, 30, 40, 50, 60, 75, 103, 175]\n",
    "cs_vals = [4,5,6]# set the set of models and rates we seek to explore\n",
    "model_descriptions =[]\n",
    "cv_folds = 5\n",
    "\n",
    "best_accuracy = -1\n",
    "best_pca_cv_model = None\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataframe to represent accuracy for each model\n",
    "\n",
    "for pca_cnt in pca_cnts:\n",
    "    model_description = str(pca_cnt) + \"_PCA\"\n",
    "    model_descriptions.append(model_description)\n",
    "\n",
    "pca_cnt_cv_acc_df = pd.DataFrame(\n",
    "    index=model_descriptions,\n",
    "    columns=['Train_accuracy', 'Test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_counter = 0\n",
    "for pca_cnts in pca_cnts:\n",
    "    # adding cross validation and setting c high to avoid additional regularizatoin\n",
    "    logreg =  LogisticRegressionCV(Cs=cs_vals, solver='liblinear', max_iter=1000, cv=cv_folds)\n",
    "    # note we're using the pcaX dataset, subsetted to include the specific feature nr.\n",
    "    logreg.fit(pcaX_df_train.iloc[:,:pca_cnts], y_train)\n",
    "    \n",
    "    y_hat_train = logreg.predict(pcaX_df_train.iloc[:,:pca_cnts])\n",
    "    train_accuracy = accuracy_score(y_train.to_numpy(), y_hat_train)\n",
    "\n",
    "    y_hat_test = logreg.predict(pcaX_df_test.iloc[:,:pca_cnts])\n",
    "    test_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "\n",
    "    # adding accuracy to a list in case we want to show how accuracy changes with pca_cnts\n",
    "    accuracies.append(cur_accuracy)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_pca_model = logreg\n",
    "        best_pca_cnt = pca_cnts\n",
    "    \n",
    "    # Add rates to dataframe for clear comparison \n",
    "    curr_model_descr = model_descriptions[iteration_counter]\n",
    "    pca_cnt_cv_acc_df.loc[curr_model_descr,'Train_accuracy'] = train_accuracy\n",
    "    pca_cnt_cv_acc_df.loc[curr_model_descr,'Test_accuracy'] = test_accuracy\n",
    "    iteration_counter = iteration_counter + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rates by model in percentage format\n",
    "print(\"Comparing different regression models by varying the number of principal components with cross validation:\")\n",
    "pca_cnt_cv_acc_df.style.format({\n",
    "    'Train_accuracy': '{:,.1%}'.format,\n",
    "    'Test_accuracy': '{:,.1%}'.format,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy is\",best_accuracy, \"with\", best_pca_cnt, \"components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret what the model says about the relationship between your best_predictor and Cancer_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the PCR\n",
    "scaler = sk.preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# @sunil not sure if we need this\n",
    "# Z = scaler.transform(X_train)\n",
    "pca = PCA(n_components=best_pca_cnt).fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pcr = []\n",
    "pcrxz = []\n",
    "iter = 0\n",
    "# Run through each of the principle components, and grab the coeficients for the original predictors\n",
    "for pca_cnt in range(best_pca_cnt):\n",
    "    pca_cnt = pca_cnt+1\n",
    "    logit_pcr.append(LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df_train.iloc[:,:pca_cnt],y_train))\n",
    "    pcrxz.append(logit_pcr[iter].coef_*np.transpose(pca.components_[0:1,:]).sum(axis=1))\n",
    "\n",
    "\n",
    "# Add up components to produce the coefficent for best_predictor\n",
    "resultsZ = np.vstack((pcrxz))\n",
    "coef_sum = resultsZ.sum(axis=0)\n",
    "print(\"The coefficient for the best predictor is:\", coef_sum[best_pred_pos])\n",
    "# convert log odds into odds for the coeffinet associated with the best predictor\n",
    "odds = np.exp(coef_sum[best_pred_pos])\n",
    "print(\"The odds for the best predictor are:\",odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Sunil. What do you think of this interpretation? This type of explanation is also used higher up.\n",
    "**Comment**\n",
    "Coefficient interpretation: For every one-unit increase in the best predictor 'Y08612_at', the odds that the observation is in AML are roughly equal to the odds that the observation is not in AML, when all other variables are held constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 6 [15 pts]: Evaluating Classifiers </b></div>\n",
    "\n",
    "**6.1**: Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).\n",
    "\n",
    "**6.2** Evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models using AUC.  That is calculate AUC for the following models:\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "For the model with the best AUC, plot the ROC. Briefly interpret your plot.\n",
    "\n",
    "**6.3** Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?\n",
    "\n",
    "*See extra information about ALL and AML at the bottom of this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristics (FOC) curve illustrates the trade-off between True Positive Rate - how much correctly classified as 1 = and False Positive Rate (How much incorrectly classified as 1).\n",
    "\n",
    "The ROC curve illustrates a range of possible cut off rates. A curve closer to the top left corner indicates a better classifier/Model.\n",
    "\n",
    "AUC is the Area Under the Curve in the ROC plot. A higher AUC is better. AUC=1 means all positive examples come after your negative example. AUC = 0 means all negative examples come after your positive example. AUC=0.5 means a random classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 use AUC to evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models.  That is calculate AUC for the following models:**\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "**For the model with the best AUC, plot the ROC. Briefly interpret your plot.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataframe to represent accuracy for each model\n",
    "model_descriptions = ['Best logistic model', 'Best kNN model', 'Best PCA model']\n",
    "auc_by_model_df = pd.DataFrame(\n",
    "    index=model_descriptions,\n",
    "    columns=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Best logisic model\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, best_reg_model.predict_proba(X_test)[:,1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "# add auc to table\n",
    "auc_by_model_df.loc['Best logistic model','AUC'] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Best Knn model\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, best_knn_model.predict_proba(X_test)[:,1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "# add auc to table\n",
    "auc_by_model_df.loc['Best kNN model','AUC'] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Best PCA model\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, best_pca_model.predict_proba(pcaX_df_test.iloc[:,:2])[:,1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "# add auc to table\n",
    "auc_by_model_df.loc['Best PCA model','AUC'] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rates by model in percentage format\n",
    "print(\"Comparing AUC's between different type of models:\")\n",
    "auc_by_model_df.style.format({\n",
    "    'AUC': '{:,.1%}'.format,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#Make ROC curves to evaluate a model's overall useability.\n",
    "#####\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# a function to make 'pretty' ROC curves for this model\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "        \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2, mutation_scale =1\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05]) \n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Sunil This doesn't look great, but the example from teh lab looks the same on my computer\n",
    "make_roc(\"Logistic\", best_reg_model, y_test, X_test, ax=None, labe=5, proba=True, skip=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** We can observe that the model is pretty good. The ROC curve goes up steeply. The model can discern most true positives before it starts identifying false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3 Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** Based on AUC, there is no clear stand-out winner. The logistic regression and KNN models both have a similar and excellent AUC score (at 87%). At 83%, the PCA model performs only marginally worse.\n",
    "\n",
    "Since all the models work relatively well, we could consider using multiple models to predict real cancer patients. If all three models predict the same cancer type, our confidence in the prediction would increase. Nevertheless, we should not feel too confident as false positives remain and the base prevalence for leuchemia is relatively low (~14 per 100,000). The prediction should be confirmed with a higher-confidence test, such as a biopsy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Information**\n",
    "\n",
    "Acute Lymphoblastic Leukemia (ALL):\n",
    "- About 98% of children with ALL go into remission within weeks after starting treatment.\n",
    "- About 90% of those children can be cured. Patients are considered cured after 10 years in remission.\n",
    "\n",
    "Acute Myeloid Leukemia (AML):\n",
    "- In general, children with AML are seen as lower risk than adults. \n",
    "- Around 85 to 90 percent of children with AML will go into remission after induction, according to the American Cancer Society. AML will return in some cases.  \n",
    "- The five-year-survival-rate for children with AML is 60 to 70 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
